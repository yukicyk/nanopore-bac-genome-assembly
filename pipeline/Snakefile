
# =========================
# File: pipeline/Snakefile
# =========================
# Top of Snakefile — robust repo anchoring via workflow.basedir
from pathlib import Path
# Snakemake sets workflow.basedir to the directory containing this Snakefile
print("[DEBUG workflow.basedir] " + str(workflow.basedir))

# workflow.basedir is the directory that contains this Snakefile, i.e., <repo>/pipeline
PIPELINE_DIR = Path(workflow.basedir).resolve()
REPO = PIPELINE_DIR.parent

WF_DIR = PIPELINE_DIR   
CFG_DIR = REPO / "config"
SCRIPTS_DIR = REPO / "scripts"
RULES_DIR = PIPELINE_DIR / "rules"
include: str(RULES_DIR / "io.smk")
include: str(RULES_DIR / "validate_manifest.smk")

print("[DEBUG REPO] " + str(REPO))

# Optional sanity: ensure we’re inside your home/projects and not site-packages
assert "site-packages" not in str(REPO), "Unexpected site-packages REPO: " + str(REPO)

# If you load config:
CONFIG = CFG_DIR / "config.yaml"

# Helper to read nested config keys even if `config` is None
def cfg(path, default=None):
    # path like "fetch.enable"
    try:
        d = config  # may be None during parse
    except NameError:
        return default
    if not isinstance(d, dict):
            return default
    keys = path.split(".")
    cur = d
    for i, k in enumerate(keys):
        if not isinstance(cur, dict):
            return default
        cur = cur.get(k, default if i == len(keys) - 1 else {})
    return cur if cur is not None else default
        
# --------------------------------------------------------------------
# Fetch / samples TSV configuration (now that cfg() is safe)
# --------------------------------------------------------------------
FETCH_ENABLE = bool(cfg("fetch.enable", True))
FETCH_NON_INTERACTIVE = bool(cfg("fetch.non_interactive", False))
FETCH_THREADS = int(cfg("fetch.threads", 4))
FETCH_SCRIPT = cfg("fetch.script", str(SCRIPTS_DIR / "fetch_or_prompt.py"))
FETCH_NON_INTERACTIVE_FLAG = "--non-interactive" if FETCH_NON_INTERACTIVE else ""
FETCH_ENABLE_STR = "true" if FETCH_ENABLE else "false"

# Where to read/write the resolved samples TSV
SAMPLES_TSV_IN = (REPO / cfg("fetch.samples", "config/samples.tsv")).resolve()
SAMPLES_TSV_OUT = (REPO / cfg("fetch.out", "config/samples.resolved.tsv")).resolve()

# Debug print to verify paths
print("Resolved paths:")
print(f"  REPO        = {REPO}")
print(f"  CONFIG      = {CONFIG} (exists={CONFIG.exists()})")
print(f"  SAMPLES_IN  = {SAMPLES_TSV_IN} (exists={SAMPLES_TSV_IN.exists()})")
print(f"  SAMPLES_OUT = {SAMPLES_TSV_OUT}")
print(f"  SCRIPTS_DIR = {SCRIPTS_DIR} (exists={SCRIPTS_DIR.exists()})")


# --------------------------------------------------------------------
# Ensure helper exists (self-contained delivery)
# --------------------------------------------------------------------
HELPER_SCRIPT = SCRIPTS_DIR / "read_samples_tsv.py"
HELPER_SCRIPT.parent.mkdir(parents=True, exist_ok=True)
if not HELPER_SCRIPT.exists():
    HELPER_SCRIPT.write_text("""#!/usr/bin/env python3
import csv, sys, json
from pathlib import Path

def main(tsv_path):
    samples = []
    p = Path(tsv_path)
    if not p.exists():
        print(json.dumps({'error': f'not found: {p}'}))
        return
    with p.open('r', newline='') as fh:
        reader = csv.DictReader(fh, delimiter='\\t')
        required = {'sample_id','platform','read_path'}
        headers = [h.strip() for h in (reader.fieldnames or [])]
        missing = required - set(headers)
        if missing:
            print(json.dumps({'error': f'missing columns: {sorted(missing)}'}))
            return
        for row in reader:
            sample = (row.get('sample_id') or '').strip()
            plat = (row.get('platform') or '').strip().lower()
            rpath = (row.get('read_path') or '').strip()
            if not sample:
                continue
            samples.append({'sample_id': sample, 'platform': plat, 'read_path': rpath})
    print(json.dumps({'samples': samples}))

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(json.dumps({'error': 'usage: read_samples_tsv.py <samples.tsv>'}))
    else:
        main(sys.argv[1])
""")
    os.chmod(HELPER_SCRIPT, 0o755)

# --------------------------------------------------------------------
# Include IO helpers that define:
#   - read_samples(tsv_path) -> [sample_ids]
#   - raw_read_path(sample_id) -> path
# Must not error at parse time if files are missing.
# --------------------------------------------------------------------



# --------- Lazy helpers (no file access at parse time) ----------
def samples_list():
    # Use the resolved TSV, produced by resolve_samples
    return read_samples(str(SAMPLES_TSV_OUT))

def raw_reads_dict():
    # Map sample -> read path at job expansion time
    return {s: raw_read_path(s) for s in samples_list()}
 
# =========================
# Include manifest validator
# =========================



# Optional: expose a high-level target
rule validate_manifests:
    input:
        "reports/manifest_validation.txt",
        "reports/manifest_validation.json"
    message:
        "Manifest validation reports ready."


# ---------------------------------
# Rule: resolve_samples
# ---------------------------------
# Produce SAMPLES_TSV_OUT by either copying (if disabled) or fetching to fill read_path.
rule resolve_samples:
    input:
        samples=str(SAMPLES_TSV_IN)
    output:
        out=str(SAMPLES_TSV_OUT)
    threads:
        FETCH_THREADS
    conda:
        "envs/fetch.yaml"
    message:
        "Resolving/Fetching FASTQs -> {output.out}"
    shell:
        r"""
        set -euo pipefail
        if [ "{FETCH_ENABLE_STR}" = "true" ]; then
            python {FETCH_SCRIPT} \
              --samples {input.samples} \
              --out {output.out} \
              --threads {threads} \
              --non-interactive \
              --skip-existing \
              --outdir data/raw 
        else
            if [ "{input.samples}" != "{output.out}" ]; then
                mkdir -p "$(dirname {output.out})"
                cp -f {input.samples} {output.out}
            fi
        fi
        """
# -------------------------------
# quick header check for samples.tsv
# -------------------------------
rule check_samples_headers:
    input:
        str(SAMPLES_TSV_OUT)
    output:
        touch("reports/samples_header_check.ok")
    message:
        "Checking samples TSV headers via helper"
    run:
        import json, subprocess
        p = subprocess.run([str(HELPER_SCRIPT), str(input[0])], capture_output=True, text=True, check=True)
        data = json.loads(p.stdout or "{}")
        if "error" in data:
            raise ValueError(f"samples.tsv header error: {data['error']}")
        Path("reports").mkdir(parents=True, exist_ok=True)
        Path(output[0]).touch()

# -------------------------------
# QC rules (NanoPlot + validation)
# -------------------------------
# FASTQ integrity: validate each gzipped FASTQ before QC, work on thee resolved TSV

rule validate_fastq_gz:
    input:
        # Ensure fetch step runs first
        fastq=lambda wc: raw_reads_dict()[wc.sample],
        samples_tsv=str(SAMPLES_TSV_OUT)
    output:
        touch("results/qc/{sample}/.validated")
    conda:
        "envs/ont-qc.yaml"
    threads: 1
    message:
        "Validating gzip for {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail
        zcat -t {input.fastq} >/dev/null
        mkdir -p $(dirname {output})
        touch {output}
        """

# Make NanoPlot QC per-sample depends on validation
rule nanoplot_qc:
    input:
        reads=lambda wc: raw_reads_dict()[wc.sample],
        validated="results/qc/{sample}/.validated"
    output:
        nanoplot_html=touch("results/qc/{sample}/nanoplot/NanoPlot-report.html"),
        nanoplot_json=touch("results/qc/{sample}/nanoplot/NanoPlot-data.json"),
        nanoplot_log=touch("results/qc/{sample}/nanoplot/NanoPlot-log.txt"),
        readlengths=touch("results/qc/{sample}/nanoplot/readlengths.png"),
        qlen=touch("results/qc/{sample}/nanoplot/qualityLength.png"),
    conda:
        "envs/ont-qc.yaml"
    threads: 4
    message:
        "Running NanoPlot QC for {wildcards.sample}"
    shell:
        r"""
        outdir="results/qc/{wildcards.sample}/nanoplot"
        mkdir -p "$outdir"
        NanoPlot \
          --fastq {input.reads} \
          --outdir "$outdir" \
          --threads {threads} \
          --verbose \
          --maxlength 2000000 \
          --nocolor \
          2> "$outdir/NanoPlot-log.txt"

        # Normalize outputs across NanoPlot versions
        : > "$outdir/NanoPlot-report.html"
        : > "$outdir/NanoPlot-data.json"
        : > "$outdir/qualityLength.png"
        : > "$outdir/readlengths.png"
        """

# Multi-sample index linking to each NanoPlot report
rule qc_index:
    input:
        # Build list from current samples list; the per-sample htmls depend on validate -> resolve_samples
        str(SAMPLES_TSV_OUT),
        expand("results/qc/{sample}/nanoplot/NanoPlot-report.html", sample=samples_list())
    output:
        "results/qc/index.html"
    message:
        "Building QC index"
    run:
        out = []
        out.append("<!doctype html><meta charset='utf-8'><title>QC index</title>")
        out.append("<h1>QC reports</h1><ul>")
        for s in samples_list():
            out.append(f"<li><a href='{s}/nanoplot/NanoPlot-report.html'>{s}</a></li>")
        out.append("</ul>")
        Path("results/qc").mkdir(parents=True, exist_ok=True)
        Path("results/qc/index.html").write_text("\n".join(out))

# -------------------------------
# Default target
# -------------------------------
rule all:
    input:
        # Ensure resolution happens, then downstream products
        str(SAMPLES_TSV_OUT),
        # Ensure validation reports are produced early
        "reports/manifest_validation.txt",
        "reports/manifest_validation.json",
        # Ensure all samples have their QC reports
        # Primary QC outputs
        "results/qc/index.html"


# Debuggind for the missing conda problem
#rule debug_conda:
#    output: "tmp/debug_conda.txt"
#    shell:
#        r"""
#        set -x
#        echo "PATH=$PATH"
#        which conda || true
#        ${{SNAKEMAKE_CONDA:-conda}} info --json | head -c 120 || true
#        echo OK > {output}
#        """